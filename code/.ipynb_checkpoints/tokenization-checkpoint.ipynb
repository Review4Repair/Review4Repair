{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unified_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import MWETokenizer, TweetTokenizer, WordPunctTokenizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "VALUES = ['>>>=', '>>=', '<<=',  '%=', '^=', '|=', '&=', '/=',\n",
    "                  '*=', '-=', '+=', '<<', '--', '++', '||', '&&', '!=',\n",
    "                  '>=', '<=', '==', '%', '^', '|', '&', '/', '*', '-',\n",
    "                  '+', ':', '?', '~', '!', '<', '>', '=', '...', '->', '::', '\\\\', '\\\\\\\\', '\\*', '*\\\\', '\\\\\\\\\\\\']\n",
    "INFIX = ['||', '&&', '|', '^', '&', '==', '!=', '<', '>', '<=', '>=', \n",
    "             '<<', '>>', '>>>', '+', '-', '*', '/', '%']\n",
    "PREFIX = ['++', '--', '!', '~', '+', '-']\n",
    "POSTFIX = ['++', '--']\n",
    "ASSIGNMENT = ['=', '+=', '-=', '*=', '/=', '&=', '|=', '^=', '%=', '<<=', '>>=', '>>>=']\n",
    "LAMBDA = ['->']\n",
    "COMMENT = ['//', '/*', '*/']\n",
    "METHOD_REFERENCE = ['::']\n",
    "\n",
    "WHITESPACE_DICT = {\"                \":'<|16-s|>', \n",
    "                   \"            \":'<|12-s|>', \n",
    "                   \"        \":'<|8-s|>', \n",
    "                   \"    \":'<|4-s|>', \n",
    "                   \"  \":'<|2-s|>', \" \":'<|s|>',\n",
    "                   \"\\t\\t\\t\\t\":'<|4-t|>',\"\\t\\t\\t\":'<|3-t|>',\"\\t\\t\":'<|2-t|>',\"\\t\":'<|t|>',\"\\n\":'<|nl|>'}\n",
    "\n",
    "WHITESPACES = [WHITESPACE_DICT[x] for x in WHITESPACE_DICT]\n",
    "\n",
    "REVERSE_WHITESPACE_DICT = {}\n",
    "for key in WHITESPACE_DICT:\n",
    "    value = WHITESPACE_DICT[key]\n",
    "    REVERSE_WHITESPACE_DICT[value] = key\n",
    "\n",
    "CUSTOM_TOKEN = [\"<|startcode|>\", \"<|endcode|>\", \"<|startfocus|>\", \"<|endfocus|>\", \"<|startcomment|>\", \"<|endcomment|>\", \n",
    "                   \"<|stringliteral|>\", \"<|singlelinecomment|>\", \"<|multilinecomment|>\", \"<|del|>\"]\n",
    "\n",
    "values = list(set(INFIX+PREFIX+POSTFIX+ASSIGNMENT+LAMBDA+COMMENT+METHOD_REFERENCE))\n",
    "token_phrases = []\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "for tok in CUSTOM_TOKEN+WHITESPACES:\n",
    "    temp = tuple(word_punct_tokenizer.tokenize(tok))\n",
    "    token_phrases.append(temp)\n",
    "for w in values:\n",
    "    temp = tuple(MWETokenizer().tokenize(w))\n",
    "    if len(temp) > 1:\n",
    "        token_phrases.append(temp)\n",
    "\n",
    "word_punct_tokenizer = WordPunctTokenizer()\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "mwe_tokenizer = MWETokenizer(token_phrases, separator=\"\")\n",
    "\n",
    "def state(c):\n",
    "    n = ord(c)\n",
    "    if n>=97 and n<=122: # lower case\n",
    "        return 1\n",
    "    elif n>=65 and n<=90: # upper case\n",
    "        return 2\n",
    "    elif n>=48 and n<=57: # numbers\n",
    "        return 3\n",
    "    elif c.isspace(): # whitespaces\n",
    "        return 4\n",
    "    elif c in ['_', '$']: \n",
    "        return 5\n",
    "    elif n < 128:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7\n",
    "\n",
    "def space_up(s):\n",
    "    if s is None or s == \"\":\n",
    "        return \"\"\n",
    "    new_s = s[0]\n",
    "    for i in range(1,len(s)):\n",
    "        prev_state = state(s[i-1])\n",
    "        curr_state = state(s[i])\n",
    "        if prev_state in [1,2] and curr_state in [3]:\n",
    "            new_s += \" \"\n",
    "        elif prev_state in [1] and curr_state in [2]:\n",
    "            new_s += \" \"\n",
    "        elif prev_state in [3] and curr_state in [1,2]:\n",
    "            new_s += \" \"\n",
    "        elif prev_state in [1,2,3] and curr_state in [5]:\n",
    "            new_s += \" \"\n",
    "        elif prev_state in [5] and curr_state in [1,2,3]:\n",
    "            new_s += \" \"\n",
    "        new_s+=s[i]\n",
    "    return new_s\n",
    "\n",
    "def white_space_tokenize(s):\n",
    "    for x in WHITESPACE_DICT:\n",
    "        s = s.replace(x, WHITESPACE_DICT[x])\n",
    "    for key in REVERSE_WHITESPACE_DICT:\n",
    "        #val = REVERSE_WHITESPACE_DICT[x]\n",
    "        s = s.replace(key, \" \"+key+\" \")\n",
    "    return s\n",
    "\n",
    "def hard_tokenization(comment):\n",
    "    comment = white_space_tokenize(comment)\n",
    "    comment = space_up(comment)\n",
    "    tokenized = tweet_tokenizer.tokenize(comment)\n",
    "    tokenized = word_punct_tokenizer.tokenize(' '.join(tokenized))\n",
    "    tokenized = mwe_tokenizer.tokenize(tokenized)\n",
    "    tokenized_comment = ' '.join(tokenized)\n",
    "    tokenized_comment = re.sub(r'[^\\x00-\\x7f]',r'', tokenized_comment)\n",
    "    tokenized = tokenized_comment.split()\n",
    "    return tokenized\n",
    "\n",
    "def hard_detokenization(tokens):\n",
    "    s = \"\"\n",
    "    for token in tokens:\n",
    "        if token in REVERSE_WHITESPACE_DICT:\n",
    "            s+= REVERSE_WHITESPACE_DICT[token]\n",
    "        else:\n",
    "            s+= token\n",
    "    return s\n",
    "\n",
    "\n",
    "def soft_detokenize(tokens):\n",
    "    new_tokens = ['#']\n",
    "    whitespace_on = 0\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        if state(token[0]) in [1,2,3,5] and state(new_tokens[-1][-1]) in [1,2,3,5] and not whitespace_on:\n",
    "            new_tokens[-1]+=token\n",
    "            whitespace_on = 0\n",
    "        elif token in REVERSE_WHITESPACE_DICT:\n",
    "            whitespace_on = 1\n",
    "            continue\n",
    "        else:\n",
    "            new_tokens.append(token)\n",
    "            whitespace_on = 0\n",
    "            \n",
    "    return new_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/amllab-2070/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from os import path\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from threading import Lock\n",
    "lock = Lock()\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def read(path):\n",
    "    return open(path, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "\n",
    "BASE_code=\"/media/amllab-2070/WIN_10/research_stuff/codes/\"\n",
    "\n",
    "\n",
    "def end_scope(start_index, lines):\n",
    "    #print(lines)\n",
    "    counter = 0\n",
    "    end_index = -1\n",
    "    found = False\n",
    "    for index in range(start_index, len(lines), 1):\n",
    "        #print(lines[index])\n",
    "        #print(counter)\n",
    "        for char in lines[index]:\n",
    "            if(char=='{'):\n",
    "                counter+=1\n",
    "                found = True\n",
    "            elif(char == \"}\"):\n",
    "                counter-=1\n",
    "                found = True\n",
    "            if(counter==0 and found):\n",
    "                end_index = index\n",
    "                break\n",
    "        if end_index!=-1:\n",
    "            break\n",
    "    return end_index\n",
    "\n",
    "pretrained_data_processed = []\n",
    "MAX_NUM_TOKEN = 400\n",
    "UP_TOKEN = MAX_NUM_TOKEN//2\n",
    "c = 0\n",
    "ec = 0\n",
    "errors = []\n",
    "\n",
    "def process(data):\n",
    "    if \"<|sep|>\" not in data:\n",
    "        return\n",
    "    lst = data.split('<|sep|>')\n",
    "    status = lst[0]\n",
    "    main_code=lst[1]\n",
    "    target = lst[2]\n",
    "    sample = main_code\n",
    "    #print(sample)\n",
    "    process.counter +=1\n",
    "    start_fc = 0\n",
    "    end_fc = 0\n",
    "    for idx, line in enumerate(sample.split('\\n')):\n",
    "        if \"startfocus\" in line:\n",
    "            start_fc = idx\n",
    "        if \"endfocus\" in line:\n",
    "            end_fc = idx\n",
    "    \n",
    "    #print(main_code)\n",
    "    line_no = (start_fc+end_fc)//2\n",
    "    data_dic = {}\n",
    "    data_dic['status'] = status\n",
    "    data_dic['target'] = target\n",
    "    \n",
    "    func_list = []\n",
    "    data_dic['code_snippet'] = main_code\n",
    "    data_dic['prime_var_dic'] = {}\n",
    "    classes = re.findall(r\"(?:(public\\s))?(class)\\s([^\\n\\s]*)\", sample)\n",
    "    lines = sample.split('\\n')\n",
    "    class_list = []\n",
    "    sp_start_func= -1\n",
    "    sp_end_func= len(sample.split('\\n'))   \n",
    "    for cls_tpl in classes:\n",
    "        s = \"\".join(cls_tpl[-1])\n",
    "        s = s.strip()\n",
    "        class_list.append(s) ## class list\n",
    "        cls = s\n",
    "        #print(s)\n",
    "        start_index = -1\n",
    "        for i in range(len(lines)):\n",
    "            if cls in lines[i]:\n",
    "                start_index = i\n",
    "                break\n",
    "        #print(start_index)\n",
    "        end_index = end_scope(start_index, lines)\n",
    "        #print(end_index)\n",
    "        class_scope_dic = {}\n",
    "        for i in range(start_index, end_index+1):\n",
    "            class_scope_dic[i] = True\n",
    "        funcs = re.findall(r\"(public|protected|private|static|\\s) +([\\w\\<\\>\\[\\]]+\\s+(\\w+)) *\\([^\\)]*\\) *(\\{?|[^;])\", \"\\n\".join(lines[start_index:end_index+1]))\n",
    "        func_scopes = []\n",
    "        #print(funcs)\n",
    "        #continue\n",
    "        visited_func = set()\n",
    "        for func in funcs:\n",
    "            #print(func)\n",
    "            if(func[-1]!= \"{\"):\n",
    "                continue\n",
    "            fc = list(func)\n",
    "            fc = fc[-3]\n",
    "            s = \"\".join(fc)\n",
    "            s = s.strip()\n",
    "            #func_list.append(s) ## function list\n",
    "            #print(func_list)\n",
    "            \n",
    "            for index in range(start_index, end_index+1, 1):\n",
    "                if s in lines[index]:\n",
    "                    start_func = index\n",
    "                    end_func = end_scope(index, lines)\n",
    "                    \n",
    "                    func_scopes.append((start_func,end_func))\n",
    "                    if s not in visited_func:\n",
    "                        visited_func.add(s)\n",
    "                        func_list.append(lines[index].replace(\"{\", \"\").strip())\n",
    "\n",
    "\n",
    "        for func_sc in func_scopes:\n",
    "            for i in range(func_sc[0], func_sc[1]+1):\n",
    "                class_scope_dic[i] = False\n",
    "        prime_var_list = []\n",
    "        dic_vars = {}\n",
    "        for i in range(start_index, end_index+1):\n",
    "            if class_scope_dic[i]==True:\n",
    "                #prime_vars = re.findall(r\"\"\"\"[^\"]*\"|((?=_[a-z_0-9]|[a-z])[a-z_0-9]+((?=\\s*=)))\"\"\",lines[i])\n",
    "                if '(' not in lines[i] and \"return\" not in lines[i] and \"extends\" not in lines[i]:\n",
    "                    prime_vars = re.findall(r\"\"\"(\\w+\\s+)([a-zA-Z_][a-zA-Z0-9_]*)\"\"\", lines[i])\n",
    "                    if len(prime_vars)==2:\n",
    "                        if(len(prime_vars[1])==2 and prime_vars[1][1] in dic_vars.keys()):\n",
    "                            dic_vars[prime_vars[1][1]]+=1\n",
    "                        elif len(prime_vars[1])==2:\n",
    "                            dic_vars[prime_vars[1][1]] =1\n",
    "                            \n",
    "        data_dic['prime_var_dic'] = dic_vars\n",
    "    data_dic['class_list'] = class_list\n",
    "    data_dic['func_list'] = func_list\n",
    "    \n",
    "    up_count = 0\n",
    "    up_done = False\n",
    "    down_count = 0\n",
    "    down_done = False\n",
    "    splitted = sample.split(\"\\n\")\n",
    "    while(1):\n",
    "        #print(max(0, line_no-up_count),line_no+1)\n",
    "        if len(hard_tokenization(\"\\n\".join(splitted[max(0,sp_start_func, line_no-up_count):line_no+1])))<UP_TOKEN:\n",
    "            if(up_count==line_no):\n",
    "                break\n",
    "            up_count+=1\n",
    "        else:\n",
    "            up_count-=1\n",
    "            break\n",
    "    while(1):\n",
    "        if len(hard_tokenization(\"\\n\".join(splitted[max(0,sp_start_func, line_no-up_count):min(line_no+down_count+1,sp_end_func, len(splitted))])))<MAX_NUM_TOKEN:\n",
    "            #print(len(hard_tokenization(\"\\n\".join(splitted[(line_no-up_count):min(line_no+down_count+1,sp_end_func, len(splitted))]))))\n",
    "            #print(MAX_NUM_TOKEN)\n",
    "            if(down_count==len(splitted)):\n",
    "                break\n",
    "            down_count+=1\n",
    "        else:\n",
    "            down_count-=1\n",
    "            break\n",
    "    #print(line_no+down_count+1,sp_end_func, len(splitted), up_count)\n",
    "    code_snippet = \"\\n\".join(splitted[max(0,sp_start_func, line_no-up_count):min(line_no+down_count+1,sp_end_func, len(splitted))])\n",
    "    #print(\"length = \", len(hard_tokenization(code_snippet)))\n",
    "    \n",
    "    if \"<|startfocus|>\" in code_snippet and \"<|endfocus|>\" not in code_snippet:\n",
    "        code_snippet =code_snippet+  \"\\n<|endfocus|>\"\n",
    "    if \"<|startfocus|>\" not in code_snippet and \"<|endfocus|>\" in code_snippet:\n",
    "        code_snippet = \"<|startfocus|>\\n\"+code_snippet\n",
    "        \n",
    "    data_dic['tokenized_code_snippet'] = hard_tokenization(code_snippet)#data_dic['code_snippet'])\n",
    "    data_dic['tokenized_target'] = hard_tokenization(data_dic['target'])\n",
    "    data_dic['code_snippet'] = code_snippet\n",
    "    \n",
    "    data_dic['global_index'] = process.counter\n",
    "    #print(code_snippet)\n",
    "    #print(process.counter)\n",
    "    if \"<|startfocus|>\" in code_snippet and \"<|endfocus|>\" in code_snippet:\n",
    "        #print(process.counter)\n",
    "        with lock:\n",
    "            pretrained_data_processed.append(data_dic)\n",
    "            if(len(pretrained_data_processed)%100==0):\n",
    "                print(\"data size = \", len(pretrained_data_processed))\n",
    "            if len(pretrained_data_processed)>100000:\n",
    "                print(\"saving data in pretrained_v1/pretrained_data_processed_v1_file_no_{}.json\".format(process.file_index))\n",
    "                with open('pretrained_v1/pretrained_data_processed_v1_file_no_{}.json'.format(process.file_index), 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "                    json.dump(pretrained_data_processed, f)\n",
    "                pretrained_data_processed.clear()\n",
    "                process.file_index+=1\n",
    "                gc.collect()\n",
    "            \n",
    "            \n",
    "def get_source_target(file1, file2, max_target_len = 5, max_input_len=20):\n",
    "    difftype = 'alldiff'\n",
    "    source_target = subprocess.check_output(['java', '-jar', 'AllDIff.jar',\\\n",
    "                                     difftype,file1, file2, str(max_target_len), \\\n",
    "                                     str(max_input_len)]).decode('utf-8')\n",
    "    return source_target.replace(\"alldiff\", \"\")\n",
    "\n",
    "def diff_file(file_1, file_2):\n",
    "    source_target = get_source_target(file_1,file_2)\n",
    "    for i in source_target.split('<|datasep|>'):\n",
    "        process(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,json\n",
    "import csv\n",
    "import _thread\n",
    "import threading\n",
    "import time\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def read(path):\n",
    "    return open(path, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "\n",
    "import glob \n",
    "cnt= 0\n",
    "folder_list = []\n",
    "for folder in glob.glob(BASE_code+'*'):\n",
    "    java_files = glob.glob(folder+'/*.java')\n",
    "    if len(java_files)>0:\n",
    "        folder_list.append(folder)\n",
    "\n",
    "#print(folder_list)\n",
    "file_size = len(folder_list)\n",
    "batch_size = 100\n",
    "\n",
    "def process_folder(fd):\n",
    "    files = glob.glob(fd+'/*.java')\n",
    "    #print(files)\n",
    "    for idx in range(len(files)-1):\n",
    "        diff_file(files[idx],files[idx+1])\n",
    "\n",
    "def translate(file_no):\n",
    "    for x in range(file_no*batch_size, min(file_size,(file_no+1)*batch_size)):\n",
    "        #print(folder_list[x])\n",
    "        process_folder(folder_list[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multithread Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing file =  0\n",
      "processing file =  1\n",
      "processing file =  2\n",
      "processing file =  3\n",
      "processing file =  4\n",
      "processing file =  5\n",
      "processing file =  6\n",
      "processing file =  7\n",
      "processing file =  8\n",
      "processing file =  9\n",
      "processing file =  10\n",
      "processing file =  11\n",
      "processing file =  12\n",
      "processing file =  13\n",
      "processing file =  14\n",
      "processing file =  15\n",
      "processing file =  16\n",
      "processing file =  17\n",
      "processing file =  18\n",
      "processing file =  19\n",
      "processing file =  20\n",
      "processing file =  21\n",
      "processing file =  22\n",
      "processing file =  23\n",
      "processing file =  24\n",
      "processing file =  25\n",
      "processing file =  26\n",
      "processing file =  27\n",
      "processing file =  28\n",
      "processing file =  29\n",
      "processing file =  30\n",
      "processing file =  31\n",
      "processing file =  32\n",
      "processing file =  33\n",
      "processing file =  34\n",
      "processing file =  35\n",
      "processing file =  36\n",
      "processing file =  37\n",
      "processing file =  38\n",
      "processing file =  39\n",
      "processing file =  40\n",
      "processing file =  41\n",
      "processing file =  42\n",
      "processing file =  43\n",
      "processing file =  44\n",
      "data size =  100\n",
      "processing file =  45\n",
      "processing file =  46\n",
      "processing file =  47\n",
      "processing file =  48\n",
      "processing file =  49\n",
      "processing file =  50\n",
      "processing file =  51\n",
      "processing file =  52\n",
      "processing file =  53\n",
      "processing file =  54\n",
      "processing file =  55\n",
      "processing file =  56\n",
      "processing file =  57\n",
      "processing file =  58\n",
      "processing file =  59\n",
      "processing file =  60\n",
      "processing file =  61\n",
      "processing file =  62\n",
      "processing file =  63\n",
      "processing file =  64\n",
      "processing file =  65\n",
      "processing file =  66\n",
      "processing file =  67\n",
      "processing file =  68\n",
      "processing file =  69\n",
      "processing file =  70\n",
      "processing file =  71\n",
      "processing file =  72\n",
      "processing file =  73\n",
      "processing file =  74\n",
      "processing file =  75\n",
      "processing file =  76\n",
      "processing file =  77\n",
      "processing file =  78\n",
      "processing file =  79\n",
      "processing file =  data size =  200\n",
      "80\n",
      "processing file =  81\n",
      "processing file =  82\n",
      "processing file =  83\n",
      "processing file =  84\n",
      "processing file =  85\n",
      "processing file =  86\n",
      "processing file =  87\n",
      "processing file =  88\n",
      "processing file =  89\n",
      "processing file =  90\n",
      "processing file =  91\n",
      "processing file =  92\n",
      "processing file =  93\n",
      "processing file =  94\n",
      "processing file =  95\n",
      "processing file =  96\n",
      "processing file =  97\n",
      "processing file =  98\n",
      "processing file =  99\n",
      "processing file =  100\n",
      "processing file =  101\n",
      "processing file =  102\n",
      "processing file =  103\n",
      "processing file =  104\n",
      "processing file =  105\n",
      "processing file =  106\n",
      "processing file =  107\n",
      "processing file =  108\n",
      "data size =  300\n",
      "processing file =  109\n",
      "processing file =  110\n",
      "processing file =  111\n",
      "processing file =  112\n",
      "processing file =  113\n",
      "processing file =  114\n",
      "processing file =  115\n",
      "processing file =  116\n",
      "processing file =  117\n",
      "processing file =  118\n",
      "processing file =  119\n",
      "processing file =  120\n",
      "processing file =  121\n",
      "processing file =  122\n",
      "processing file =  123\n",
      "processing file =  124\n",
      "processing file =  125\n",
      "processing file =  126\n",
      "processing file =  127\n",
      "processing file =  128\n",
      "processing file =  129\n",
      "processing file =  130\n",
      "processing file =  131\n",
      "processing file =  132\n",
      "processing file =  133\n",
      "processing file =  134\n",
      "processing file =  135\n",
      "processing file =  136\n",
      "processing file =  137\n",
      "processing file =  138\n",
      "processing file =  139\n",
      "processing file =  140\n",
      "processing file =  141\n",
      "processing file =  142\n",
      "processing file =  143\n",
      "processing file =  144\n",
      "processing file =  145\n",
      "processing file =  146\n",
      "processing file =  147\n",
      "processing file =  148\n",
      "processing file =  149\n",
      "processing file =  150\n",
      "data size =  400\n",
      "processing file =  151\n",
      "processing file =  152\n",
      "processing file =  153\n",
      "processing file =  154\n",
      "processing file =  155\n",
      "processing file =  156\n",
      "processing file =  157\n",
      "processing file =  158\n",
      "processing file =  159\n",
      "processing file =  160\n",
      "processing file =  161\n",
      "processing file =  162\n",
      "processing file =  163\n",
      "processing file =  164\n",
      "processing file =  165\n",
      "processing file =  166\n",
      "processing file =  167\n",
      "processing file =  168\n",
      "processing file =  169\n",
      "processing file =  170\n",
      "processing file =  171\n",
      "processing file =  172\n",
      "processing file =  173\n",
      "processing file =  174\n",
      "processing file =  175\n",
      "processing file =  176\n",
      "processing file =  177\n",
      "processing file =  178\n",
      "processing file =  179\n",
      "processing file =  180\n",
      "processing file =  181\n",
      "processing file =  182\n",
      "processing file =  183\n",
      "processing file =  184\n",
      "processing file =  185\n",
      "processing file =  186\n",
      "processing file =  187\n",
      "processing file =  188\n",
      "processing file =  189\n",
      "processing file =  190\n",
      "processing file =  191\n",
      "processing file =  192\n",
      "processing file =  193\n",
      "processing file = data size =  500\n",
      " 194\n",
      "processing file =  195\n",
      "processing file =  196\n",
      "processing file =  197\n",
      "processing file =  198\n",
      "processing file =  199\n",
      "processing file =  200\n",
      "processing file =  201\n",
      "processing file =  202\n",
      "processing file =  203\n",
      "processing file =  204\n",
      "processing file =  205\n",
      "processing file =  206\n",
      "processing file =  207\n",
      "processing file =  208\n",
      "processing file =  209\n",
      "processing file =  210\n",
      "processing file =  211\n",
      "processing file =  212\n",
      "processing file =  213\n",
      "processing file =  214\n",
      "processing file =  215\n",
      "processing file =  216\n",
      "processing file =  217\n",
      "processing file =  218\n",
      "processing file =  219\n",
      "processing file =  220\n",
      "processing file =  221\n",
      "processing file =  222\n",
      "processing file =  223\n",
      "processing file =  224\n",
      "processing file =  225\n",
      "processing file =  226\n",
      "processing file =  227\n",
      "data size =  600\n",
      "processing file =  228\n",
      "processing file =  229\n",
      "processing file =  230\n",
      "processing file =  231\n",
      "processing file =  232\n",
      "processing file =  233\n",
      "processing file =  234\n",
      "processing file =  235\n",
      "processing file =  236\n",
      "processing file =  237\n",
      "processing file =  238\n",
      "processing file =  239\n",
      "processing file =  240\n",
      "processing file =  241\n",
      "processing file =  242\n",
      "processing file =  243\n",
      "processing file =  244\n",
      "processing file =  245\n",
      "processing file =  246\n",
      "processing file =  247\n",
      "processing file =  248\n",
      "processing file =  249\n",
      "processing file =  250\n",
      "processing file =  251\n",
      "processing file =  252\n",
      "processing file =  253\n",
      "processing file =  254\n",
      "processing file =  255\n",
      "processing file =  256\n",
      "processing file =  257\n",
      "processing file =  258\n",
      "processing file =  259\n",
      "processing file =  260\n",
      "processing file =  261\n",
      "processing file =  262\n",
      "processing file =  263\n",
      "processing file =  264\n",
      "processing file = data size =  700\n",
      " 265\n",
      "processing file =  266\n",
      "processing file =  267\n",
      "processing file =  268\n",
      "processing file =  269\n",
      "processing file =  270\n",
      "processing file =  271\n",
      "processing file =  272\n",
      "processing file =  273\n",
      "data size =  800\n",
      "data size =  900\n",
      "data size =  1000\n",
      "data size =  1100\n",
      "data size =  1200\n",
      "data size =  1300\n",
      "data size =  1400\n",
      "data size =  1500\n",
      "data size =  1600\n",
      "data size =  1700\n",
      "data size =  1800\n",
      "data size =  1900\n",
      "data size =  2000\n",
      "data size =  2100\n",
      "data size =  2200\n",
      "data size =  2300\n",
      "data size =  2400\n",
      "data size =  2500\n",
      "data size =  2600\n",
      "data size =  2700\n",
      "data size =  2800\n",
      "data size =  2900\n",
      "data size =  3000\n",
      "data size =  3100\n",
      "data size =  3200\n",
      "data size =  3300\n",
      "data size =  3400\n",
      "data size =  3500\n",
      "data size =  3600\n",
      "data size =  3700\n",
      "data size =  3800\n",
      "data size =  3900\n",
      "data size =  4000\n",
      "data size =  4100\n",
      "data size =  4200\n",
      "data size =  4300\n",
      "data size =  4400\n",
      "data size =  4500\n",
      "data size =  4600\n",
      "data size =  4700\n",
      "data size =  4800\n",
      "data size =  4900\n",
      "data size =  5000\n",
      "data size =  5100\n",
      "data size =  5200\n",
      "data size =  5300\n",
      "data size =  5400\n",
      "data size =  5500\n",
      "data size =  5600\n",
      "data size =  5700\n",
      "data size =  5800\n",
      "data size =  5900\n",
      "data size =  6000\n",
      "data size =  6100\n",
      "data size =  6200\n",
      "data size =  6300\n",
      "data size =  6400\n",
      "data size =  6500\n",
      "data size =  6600\n",
      "data size =  6700\n",
      "data size =  6800\n",
      "data size =  6900\n",
      "data size =  7000\n",
      "data size =  7100\n",
      "data size =  7200\n",
      "data size =  7300\n",
      "data size =  7400\n",
      "data size =  7500\n",
      "data size =  7600\n",
      "data size =  7700\n",
      "data size =  7800\n",
      "data size =  7900\n",
      "data size =  8000\n",
      "data size =  8100\n",
      "data size =  8200\n",
      "data size =  8300\n",
      "data size =  8400\n",
      "data size =  8500\n",
      "data size =  8600\n",
      "data size =  8700\n",
      "data size =  8800\n",
      "data size =  8900\n",
      "data size =  9000\n",
      "data size =  9100\n",
      "data size =  9200\n",
      "data size =  9300\n",
      "data size =  9400\n",
      "data size =  9500\n",
      "data size =  9600\n",
      "data size =  9700\n",
      "data size =  9800\n",
      "data size =  9900\n",
      "data size =  10000\n",
      "data size =  10100\n",
      "data size =  10200\n",
      "data size =  10300\n",
      "data size =  10400\n",
      "data size =  10500\n",
      "data size =  10600\n",
      "data size =  10700\n",
      "data size =  10800\n",
      "data size =  10900\n",
      "data size =  11000\n",
      "data size =  11100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size =  11200\n",
      "data size =  11300\n",
      "data size =  11400\n",
      "data size =  11500\n",
      "data size =  11600\n",
      "data size =  11700\n",
      "data size =  11800\n",
      "data size =  11900\n",
      "data size =  12000\n",
      "data size =  12100\n",
      "data size =  12200\n",
      "data size =  12300\n",
      "data size =  12400\n",
      "data size =  12500\n",
      "data size =  12600\n",
      "data size =  12700\n",
      "data size =  12800\n",
      "data size =  12900\n",
      "data size =  13000\n",
      "data size =  13100\n",
      "data size =  13200\n",
      "data size =  13300\n",
      "data size =  13400\n",
      "data size =  13500\n",
      "data size =  13600\n",
      "data size =  13700\n",
      "data size =  13800\n",
      "data size =  13900\n",
      "data size =  14000\n",
      "data size =  14100\n",
      "data size =  14200\n",
      "data size =  14300\n",
      "data size =  14400\n",
      "data size =  14500\n",
      "data size =  14600\n",
      "data size =  14700\n",
      "data size =  14800\n",
      "data size =  14900\n",
      "data size =  15000\n",
      "data size =  15100\n",
      "data size =  15200\n",
      "data size =  15300\n",
      "data size =  15400\n",
      "data size =  15500\n",
      "data size =  15600\n",
      "data size =  15700\n",
      "data size =  15800\n",
      "data size =  15900\n",
      "data size =  16000\n",
      "data size =  16100\n",
      "data size =  16200\n",
      "data size =  16300\n",
      "data size =  16400\n",
      "data size =  16500\n",
      "data size =  16600\n",
      "data size =  16700\n",
      "data size =  16800\n",
      "data size =  16900\n",
      "data size =  17000\n",
      "data size =  17100\n",
      "data size =  17200\n",
      "data size =  17300\n",
      "data size =  17400\n",
      "data size =  17500\n",
      "data size =  17600\n",
      "data size =  17700\n",
      "data size =  17800\n",
      "data size =  17900\n",
      "data size =  18000\n",
      "data size =  18100\n",
      "data size =  18200\n",
      "data size =  18300\n",
      "data size =  18400\n",
      "data size =  18500\n",
      "data size =  18600\n",
      "data size =  18700\n",
      "data size =  18800\n",
      "data size =  18900\n",
      "data size =  19000\n",
      "data size =  19100\n",
      "data size =  19200\n",
      "data size =  19300\n",
      "data size =  19400\n",
      "data size =  19500\n",
      "data size =  19600\n",
      "data size =  19700\n",
      "data size =  19800\n",
      "data size =  19900\n",
      "data size =  20000\n",
      "data size =  20100\n",
      "data size =  20200\n",
      "data size =  20300\n",
      "data size =  20400\n",
      "data size =  20500\n",
      "data size =  20600\n",
      "data size =  20700\n",
      "data size =  20800\n",
      "data size =  20900\n",
      "data size =  21000\n",
      "data size =  21100\n",
      "data size =  21200\n",
      "data size =  21300\n",
      "data size =  21400\n",
      "data size =  21500\n",
      "data size =  21600\n",
      "data size =  21700\n",
      "data size =  21800\n",
      "data size =  21900\n",
      "data size =  22000\n",
      "data size =  22100\n",
      "data size =  22200\n",
      "data size =  22300\n",
      "data size =  22400\n",
      "data size =  22500\n",
      "data size =  22600\n",
      "data size =  22700\n",
      "data size =  22800\n",
      "data size =  22900\n",
      "data size =  23000\n",
      "data size =  23100\n",
      "data size =  23200\n",
      "data size =  23300\n",
      "data size =  23400\n",
      "data size =  23500\n",
      "data size =  23600\n",
      "data size =  23700\n",
      "data size =  23800\n",
      "data size =  23900\n",
      "data size =  24000\n",
      "data size =  24100\n",
      "data size =  24200\n",
      "data size =  24300\n",
      "data size =  24400\n",
      "data size =  24500\n",
      "data size =  24600\n",
      "data size =  24700\n",
      "data size =  24800\n",
      "data size =  24900\n",
      "data size =  25000\n",
      "data size =  25100\n",
      "data size =  25200\n",
      "data size =  25300\n",
      "data size =  25400\n",
      "data size =  25500\n",
      "data size =  25600\n",
      "data size =  25700\n",
      "data size =  25800\n",
      "data size =  25900\n",
      "data size =  26000\n",
      "data size =  26100\n",
      "data size =  26200\n",
      "data size =  26300\n",
      "data size =  26400\n",
      "data size =  26500\n",
      "data size =  26600\n",
      "data size =  26700\n",
      "data size =  26800\n",
      "data size =  26900\n",
      "data size =  27000\n",
      "data size =  27100\n",
      "data size =  27200\n",
      "data size =  27300\n",
      "data size =  27400\n",
      "data size =  27500\n",
      "data size =  27600\n",
      "data size =  27700\n",
      "data size =  27800\n",
      "data size =  27900\n",
      "data size =  28000\n",
      "data size =  28100\n",
      "data size =  28200\n",
      "data size =  28300\n",
      "data size =  28400\n",
      "data size =  28500\n",
      "data size =  28600\n",
      "data size =  28700\n",
      "data size =  28800\n",
      "data size =  28900\n",
      "data size =  29000\n",
      "data size =  29100\n",
      "data size =  29200\n",
      "data size =  29300\n",
      "data size =  29400\n",
      "data size =  29500\n",
      "data size =  29600\n",
      "data size =  29700\n",
      "data size =  29800\n",
      "data size =  29900\n",
      "data size =  30000\n",
      "data size =  30100\n",
      "data size =  30200\n",
      "data size =  30300\n",
      "data size =  30400\n",
      "data size =  30500\n",
      "data size =  30600\n",
      "data size =  30700\n",
      "data size =  30800\n",
      "data size =  30900\n",
      "data size =  31000\n",
      "data size =  31100\n",
      "data size =  31200\n",
      "data size =  31300\n",
      "data size =  31400\n",
      "data size =  31500\n",
      "data size =  31600\n",
      "data size =  31700\n",
      "data size =  31800\n",
      "data size =  31900\n",
      "data size =  32000\n",
      "data size =  32100\n",
      "data size =  32200\n",
      "data size =  32300\n",
      "data size =  32400\n",
      "data size =  32500\n",
      "data size =  32600\n",
      "data size =  32700\n",
      "data size =  32800\n",
      "data size =  32900\n",
      "data size =  33000\n",
      "data size =  33100\n",
      "data size =  33200\n",
      "data size =  33300\n",
      "data size =  33400\n",
      "data size =  33500\n",
      "data size =  33600\n",
      "data size =  33700\n",
      "data size =  33800\n",
      "data size =  33900\n",
      "data size =  34000\n",
      "data size =  34100\n",
      "data size =  34200\n",
      "data size =  34300\n",
      "data size =  34400\n",
      "data size =  34500\n",
      "data size =  34600\n",
      "data size =  34700\n",
      "data size =  34800\n",
      "data size =  34900\n",
      "data size =  35000\n",
      "data size =  35100\n",
      "data size =  35200\n",
      "data size =  35300\n",
      "data size =  35400\n",
      "data size =  35500\n",
      "data size =  35600\n",
      "data size =  35700\n",
      "data size =  35800\n",
      "data size =  35900\n",
      "data size =  36000\n",
      "data size =  36100\n",
      "data size =  36200\n",
      "data size =  36300\n",
      "data size =  36400\n",
      "data size =  36500\n",
      "data size =  36600\n",
      "data size =  36700\n",
      "data size =  36800\n",
      "data size =  36900\n",
      "data size =  37000\n",
      "data size =  37100\n",
      "data size =  37200\n",
      "data size =  37300\n",
      "data size =  37400\n",
      "data size =  37500\n",
      "data size =  37600\n",
      "data size =  37700\n",
      "data size =  37800\n",
      "data size =  37900\n",
      "data size =  38000\n",
      "data size =  38100\n",
      "data size =  38200\n",
      "data size =  38300\n",
      "data size =  38400\n",
      "data size =  38500\n",
      "data size =  38600\n",
      "data size =  38700\n",
      "data size =  38800\n",
      "data size =  38900\n",
      "data size =  39000\n",
      "data size =  39100\n",
      "data size =  39200\n",
      "data size =  39300\n",
      "data size =  39400\n",
      "data size =  39500\n",
      "data size =  39600\n",
      "data size =  39700\n",
      "data size =  39800\n",
      "data size =  39900\n",
      "data size =  40000\n",
      "data size =  40100\n",
      "data size =  40200\n",
      "data size =  40300\n",
      "data size =  40400\n",
      "data size =  40500\n",
      "data size =  40600\n",
      "data size =  40700\n",
      "data size =  40800\n",
      "data size =  40900\n",
      "data size =  41000\n",
      "data size =  41100\n",
      "data size =  41200\n",
      "data size =  41300\n",
      "data size =  41400\n",
      "data size =  41500\n",
      "data size =  41600\n",
      "data size =  41700\n",
      "data size =  41800\n",
      "data size =  41900\n",
      "data size =  42000\n",
      "data size =  42100\n",
      "data size =  42200\n",
      "data size =  42300\n",
      "data size =  42400\n",
      "data size =  42500\n",
      "data size =  42600\n",
      "data size =  42700\n",
      "data size =  42800\n",
      "data size =  42900\n",
      "data size =  43000\n",
      "data size =  43100\n",
      "data size =  43200\n",
      "data size =  43300\n",
      "data size =  43400\n",
      "data size =  43500\n",
      "data size =  43600\n",
      "data size =  43700\n",
      "data size =  43800\n",
      "data size =  43900\n",
      "data size =  44000\n",
      "data size =  44100\n",
      "data size =  44200\n",
      "data size =  44300\n",
      "data size =  44400\n",
      "data size =  44500\n",
      "data size =  44600\n",
      "data size =  44700\n",
      "data size =  44800\n",
      "data size =  44900\n",
      "data size =  45000\n",
      "data size =  45100\n",
      "data size =  45200\n",
      "data size =  45300\n",
      "data size =  45400\n",
      "data size =  45500\n",
      "data size =  45600\n",
      "data size =  45700\n",
      "data size =  45800\n",
      "data size =  45900\n",
      "data size =  46000\n",
      "data size =  46100\n",
      "data size =  46200\n",
      "data size =  46300\n",
      "data size =  46400\n",
      "data size =  46500\n",
      "data size =  46600\n",
      "data size =  46700\n",
      "data size =  46800\n",
      "data size =  46900\n",
      "data size =  47000\n",
      "data size =  47100\n",
      "data size =  47200\n",
      "data size =  47300\n",
      "data size =  47400\n",
      "data size =  47500\n",
      "data size =  47600\n",
      "data size =  47700\n",
      "data size =  47800\n",
      "data size =  47900\n",
      "data size =  48000\n",
      "data size =  48100\n",
      "data size =  48200\n",
      "data size =  48300\n",
      "data size =  48400\n",
      "data size =  48500\n",
      "data size =  48600\n",
      "data size =  48700\n",
      "data size =  48800\n",
      "data size =  48900\n",
      "data size =  49000\n",
      "data size =  49100\n",
      "data size =  49200\n",
      "data size =  49300\n",
      "data size =  49400\n",
      "data size =  49500\n",
      "data size =  49600\n",
      "data size =  49700\n",
      "data size =  49800\n",
      "data size =  49900\n",
      "data size =  50000\n",
      "data size =  50100\n",
      "data size =  50200\n",
      "data size =  50300\n",
      "data size =  50400\n",
      "data size =  50500\n",
      "data size =  50600\n",
      "data size =  50700\n",
      "data size =  50800\n",
      "data size =  50900\n",
      "data size =  51000\n",
      "data size =  51100\n",
      "data size =  51200\n",
      "data size =  51300\n",
      "data size =  51400\n",
      "data size =  51500\n",
      "data size =  51600\n",
      "data size =  51700\n",
      "data size =  51800\n",
      "data size =  51900\n",
      "data size =  52000\n",
      "data size =  52100\n",
      "data size =  52200\n",
      "data size =  52300\n",
      "data size =  52400\n",
      "data size =  52500\n",
      "data size =  52600\n",
      "data size =  52700\n",
      "data size =  52800\n",
      "data size =  52900\n",
      "data size =  53000\n",
      "data size =  53100\n",
      "data size =  53200\n",
      "data size =  53300\n",
      "data size =  53400\n",
      "data size =  53500\n",
      "data size =  53600\n",
      "data size =  53700\n",
      "data size =  53800\n",
      "data size =  53900\n",
      "data size =  54000\n",
      "data size =  54100\n",
      "data size =  54200\n",
      "data size =  54300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size =  54400\n",
      "data size =  54500\n",
      "data size =  54600\n",
      "data size =  54700\n",
      "data size =  54800\n",
      "data size =  54900\n",
      "data size =  55000\n",
      "data size =  55100\n",
      "data size =  55200\n",
      "data size =  55300\n",
      "data size =  55400\n",
      "data size =  55500\n",
      "data size =  55600\n",
      "data size =  55700\n",
      "data size =  55800\n",
      "data size =  55900\n",
      "data size =  56000\n",
      "data size =  56100\n",
      "data size =  56200\n",
      "data size =  56300\n",
      "data size =  56400\n",
      "data size =  56500\n",
      "data size =  56600\n",
      "data size =  56700\n",
      "data size =  56800\n",
      "data size =  56900\n",
      "data size =  57000\n",
      "data size =  57100\n",
      "data size =  57200\n",
      "data size =  57300\n",
      "data size =  57400\n",
      "data size =  57500\n",
      "data size =  57600\n",
      "data size =  57700\n",
      "data size =  57800\n",
      "data size =  57900\n",
      "data size =  58000\n",
      "data size =  58100\n",
      "data size =  58200\n",
      "data size =  58300\n",
      "data size =  58400\n",
      "data size =  58500\n",
      "data size =  58600\n",
      "data size =  58700\n",
      "data size =  58800\n",
      "data size =  58900\n",
      "data size =  59000\n",
      "data size =  59100\n",
      "data size =  59200\n",
      "data size =  59300\n",
      "data size =  59400\n",
      "data size =  59500\n",
      "data size =  59600\n",
      "data size =  59700\n",
      "data size =  59800\n",
      "data size =  59900\n",
      "data size =  60000\n",
      "data size =  60100\n",
      "data size =  60200\n",
      "data size =  60300\n",
      "data size =  60400\n",
      "data size =  60500\n",
      "data size =  60600\n",
      "data size =  60700\n",
      "data size =  60800\n",
      "data size =  60900\n",
      "data size =  61000\n",
      "data size =  61100\n",
      "data size =  61200\n",
      "data size =  61300\n",
      "data size =  61400\n",
      "data size =  61500\n",
      "data size =  61600\n",
      "data size =  61700\n",
      "data size =  61800\n",
      "data size =  61900\n",
      "data size =  62000\n",
      "data size =  62100\n",
      "data size =  62200\n",
      "data size =  62300\n",
      "data size =  62400\n",
      "data size =  62500\n",
      "data size =  62600\n",
      "data size =  62700\n",
      "data size =  62800\n",
      "data size =  62900\n",
      "data size =  63000\n",
      "data size =  63100\n",
      "data size =  63200\n",
      "data size =  63300\n",
      "data size =  63400\n",
      "data size =  63500\n",
      "data size =  63600\n",
      "data size =  63700\n",
      "data size =  63800\n",
      "data size =  63900\n",
      "data size =  64000\n",
      "data size =  64100\n",
      "data size =  64200\n",
      "data size =  64300\n",
      "data size =  64400\n",
      "data size =  64500\n",
      "data size =  64600\n",
      "data size =  64700\n",
      "data size =  64800\n",
      "data size =  64900\n",
      "data size =  65000\n",
      "data size =  65100\n",
      "data size =  65200\n",
      "data size =  65300\n",
      "data size =  65400\n",
      "data size =  65500\n",
      "data size =  65600\n",
      "data size =  65700\n",
      "data size =  65800\n",
      "data size =  65900\n",
      "data size =  66000\n",
      "data size =  66100\n",
      "data size =  66200\n",
      "data size =  66300\n",
      "data size =  66400\n",
      "data size =  66500\n",
      "data size =  66600\n",
      "data size =  66700\n",
      "data size =  66800\n",
      "data size =  66900\n",
      "data size =  67000\n",
      "data size =  67100\n",
      "data size =  67200\n",
      "data size =  67300\n",
      "data size =  67400\n",
      "data size =  67500\n",
      "data size =  67600\n",
      "data size =  67700\n",
      "data size =  67800\n",
      "data size =  67900\n",
      "data size =  68000\n",
      "data size =  68100\n",
      "data size =  68200\n",
      "data size =  68300\n",
      "data size =  68400\n",
      "data size =  68500\n",
      "data size =  68600\n",
      "data size =  68700\n",
      "data size =  68800\n",
      "data size =  68900\n",
      "data size =  69000\n",
      "data size =  69100\n",
      "data size =  69200\n",
      "data size =  69300\n",
      "data size =  69400\n",
      "data size =  69500\n",
      "data size =  69600\n",
      "data size =  69700\n",
      "data size =  69800\n",
      "data size =  69900\n",
      "data size =  70000\n",
      "data size =  70100\n",
      "data size =  70200\n",
      "data size =  70300\n",
      "data size =  70400\n",
      "data size =  70500\n",
      "data size =  70600\n",
      "data size =  70700\n",
      "data size =  70800\n",
      "data size =  70900\n",
      "data size =  71000\n",
      "data size =  71100\n",
      "data size =  71200\n",
      "data size =  71300\n",
      "data size =  71400\n",
      "data size =  71500\n",
      "data size =  71600\n",
      "data size =  71700\n",
      "data size =  71800\n",
      "data size =  71900\n",
      "data size =  72000\n",
      "data size =  72100\n",
      "data size =  72200\n",
      "data size =  72300\n",
      "data size =  72400\n",
      "data size =  72500\n",
      "data size =  72600\n",
      "data size =  72700\n",
      "data size =  72800\n",
      "data size =  72900\n",
      "data size =  73000\n",
      "data size =  73100\n",
      "data size =  73200\n",
      "data size =  73300\n",
      "data size =  73400\n",
      "data size =  73500\n",
      "data size =  73600\n",
      "data size =  73700\n",
      "data size =  73800\n",
      "data size =  73900\n",
      "data size =  74000\n",
      "data size =  74100\n",
      "data size =  74200\n",
      "data size =  74300\n",
      "data size =  74400\n",
      "data size =  74500\n",
      "data size =  74600\n",
      "data size =  74700\n",
      "data size =  74800\n",
      "data size =  74900\n",
      "data size =  75000\n",
      "data size =  75100\n",
      "data size =  75200\n",
      "data size =  75300\n",
      "data size =  75400\n",
      "data size =  75500\n",
      "data size =  75600\n",
      "data size =  75700\n",
      "data size =  75800\n",
      "data size =  75900\n",
      "data size =  76000\n",
      "data size =  76100\n",
      "data size =  76200\n",
      "data size =  76300\n",
      "data size =  76400\n",
      "data size =  76500\n",
      "data size =  76600\n",
      "data size =  76700\n",
      "data size =  76800\n",
      "data size =  76900\n",
      "data size =  77000\n",
      "data size =  77100\n",
      "data size =  77200\n",
      "data size =  77300\n",
      "data size =  77400\n",
      "data size =  77500\n",
      "data size =  77600\n",
      "data size =  77700\n",
      "data size =  77800\n",
      "data size =  77900\n",
      "data size =  78000\n",
      "data size =  78100\n",
      "data size =  78200\n",
      "data size =  78300\n",
      "data size =  78400\n",
      "data size =  78500\n",
      "data size =  78600\n",
      "data size =  78700\n",
      "data size =  78800\n",
      "data size =  78900\n",
      "data size =  79000\n",
      "data size =  79100\n",
      "data size =  79200\n",
      "data size =  79300\n",
      "data size =  79400\n",
      "data size =  79500\n",
      "data size =  79600\n",
      "data size =  79700\n",
      "data size =  79800\n",
      "data size =  79900\n",
      "data size =  80000\n",
      "data size =  80100\n",
      "data size =  80200\n",
      "data size =  80300\n",
      "data size =  80400\n",
      "data size =  80500\n",
      "data size =  80600\n",
      "data size =  80700\n",
      "data size =  80800\n",
      "data size =  80900\n",
      "data size =  81000\n",
      "data size =  81100\n",
      "data size =  81200\n",
      "data size =  81300\n",
      "data size =  81400\n",
      "data size =  81500\n",
      "data size =  81600\n",
      "data size =  81700\n",
      "data size =  81800\n",
      "data size =  81900\n",
      "data size =  82000\n",
      "data size =  82100\n",
      "data size =  82200\n",
      "data size =  82300\n",
      "data size =  82400\n",
      "data size =  82500\n",
      "data size =  82600\n",
      "data size =  82700\n",
      "data size =  82800\n",
      "data size =  82900\n",
      "data size =  83000\n",
      "data size =  83100\n",
      "data size =  83200\n",
      "data size =  83300\n",
      "data size =  83400\n",
      "data size =  83500\n",
      "data size =  83600\n",
      "data size =  83700\n",
      "data size =  83800\n",
      "data size =  83900\n",
      "data size =  84000\n",
      "data size =  84100\n",
      "data size =  84200\n",
      "data size =  84300\n",
      "data size =  84400\n",
      "data size =  84500\n",
      "data size =  84600\n",
      "data size =  84700\n",
      "data size =  84800\n",
      "data size =  84900\n",
      "data size =  85000\n",
      "data size =  85100\n",
      "data size =  85200\n",
      "data size =  85300\n",
      "data size =  85400\n",
      "data size =  85500\n",
      "data size =  85600\n",
      "data size =  85700\n",
      "data size =  85800\n",
      "data size =  85900\n",
      "data size =  86000\n",
      "data size =  86100\n",
      "data size =  86200\n",
      "data size =  86300\n",
      "data size =  86400\n",
      "data size =  86500\n",
      "data size =  86600\n",
      "data size =  86700\n",
      "data size =  86800\n",
      "data size =  86900\n",
      "data size =  87000\n",
      "data size =  87100\n",
      "data size =  87200\n",
      "data size =  87300\n",
      "data size =  87400\n",
      "data size =  87500\n",
      "data size =  87600\n",
      "data size =  87700\n",
      "data size =  87800\n",
      "data size =  87900\n",
      "data size =  88000\n",
      "data size =  88100\n",
      "data size =  88200\n",
      "data size =  88300\n",
      "data size =  88400\n",
      "data size =  88500\n",
      "data size =  88600\n",
      "data size =  88700\n",
      "data size =  88800\n",
      "data size =  88900\n",
      "data size =  89000\n",
      "data size =  89100\n",
      "data size =  89200\n",
      "data size =  89300\n",
      "data size =  89400\n",
      "data size =  89500\n",
      "data size =  89600\n",
      "data size =  89700\n",
      "data size =  89800\n",
      "data size =  89900\n",
      "data size =  90000\n",
      "data size =  90100\n",
      "data size =  90200\n",
      "data size =  90300\n",
      "data size =  90400\n",
      "data size =  90500\n",
      "data size =  90600\n",
      "data size =  90700\n",
      "data size =  90800\n",
      "data size =  90900\n",
      "data size =  91000\n",
      "data size =  91100\n",
      "data size =  91200\n",
      "data size =  91300\n",
      "data size =  91400\n",
      "data size =  91500\n",
      "data size =  91600\n",
      "data size =  91700\n",
      "data size =  91800\n",
      "data size =  91900\n",
      "data size =  92000\n",
      "data size =  92100\n",
      "data size =  92200\n",
      "data size =  92300\n",
      "data size =  92400\n",
      "data size =  92500\n",
      "data size =  92600\n",
      "data size =  92700\n",
      "data size =  92800\n",
      "data size =  92900\n",
      "data size =  93000\n",
      "data size =  93100\n",
      "data size =  93200\n",
      "data size =  93300\n",
      "data size =  93400\n",
      "data size =  93500\n",
      "data size =  93600\n",
      "data size =  93700\n",
      "data size =  93800\n",
      "data size =  93900\n",
      "data size =  94000\n",
      "data size =  94100\n",
      "data size =  94200\n",
      "data size =  94300\n",
      "data size =  94400\n",
      "data size =  94500\n",
      "data size =  94600\n",
      "data size =  94700\n",
      "data size =  94800\n",
      "data size =  94900\n",
      "data size =  95000\n",
      "data size =  95100\n",
      "data size =  95200\n",
      "data size =  95300\n",
      "data size =  95400\n",
      "data size =  95500\n",
      "data size =  95600\n",
      "data size =  95700\n",
      "data size =  95800\n",
      "data size =  95900\n",
      "data size =  96000\n",
      "data size =  96100\n",
      "data size =  96200\n",
      "data size =  96300\n",
      "data size =  96400\n",
      "data size =  96500\n",
      "data size =  96600\n",
      "data size =  96700\n",
      "data size =  96800\n",
      "data size =  96900\n",
      "data size =  97000\n",
      "data size =  97100\n",
      "data size =  97200\n",
      "data size =  97300\n",
      "data size =  97400\n",
      "data size =  97500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size =  97600\n",
      "data size =  97700\n",
      "data size =  97800\n",
      "data size =  97900\n",
      "data size =  98000\n",
      "data size =  98100\n",
      "data size =  98200\n",
      "data size =  98300\n",
      "data size =  98400\n",
      "data size =  98500\n",
      "data size =  98600\n",
      "data size =  98700\n",
      "data size =  98800\n",
      "data size =  98900\n",
      "data size =  99000\n",
      "data size =  99100\n",
      "data size =  99200\n",
      "data size =  99300\n",
      "data size =  99400\n",
      "data size =  99500\n",
      "data size =  99600\n",
      "data size =  99700\n",
      "data size =  99800\n",
      "data size =  99900\n",
      "data size =  100000\n",
      "saving data in pretrained_v1/pretrained_data_processed_v1_file_no_0.json\n",
      "data size =  100\n",
      "data size =  200\n",
      "data size =  300\n",
      "data size =  400\n",
      "data size =  500\n",
      "data size =  600\n",
      "data size =  700\n",
      "data size =  800\n",
      "data size =  900\n",
      "data size =  1000\n",
      "data size =  1100\n",
      "data size =  1200\n",
      "data size =  1300\n",
      "data size =  1400\n",
      "data size =  1500\n",
      "data size =  1600\n",
      "data size =  1700\n",
      "data size =  1800\n",
      "data size =  1900\n",
      "data size =  2000\n",
      "data size =  2100\n",
      "data size =  2200\n",
      "data size =  2300\n",
      "data size =  2400\n",
      "data size =  2500\n",
      "data size =  2600\n",
      "data size =  2700\n",
      "data size =  2800\n",
      "data size =  2900\n",
      "data size =  3000\n",
      "data size =  3100\n",
      "data size =  3200\n",
      "data size =  3300\n",
      "data size =  3400\n",
      "data size =  3500\n",
      "data size =  3600\n",
      "data size =  3700\n",
      "data size =  3800\n",
      "data size =  3900\n",
      "data size =  4000\n",
      "data size =  4100\n",
      "data size =  4200\n",
      "data size =  4300\n",
      "data size =  4400\n",
      "data size =  4500\n",
      "data size =  4600\n",
      "data size =  4700\n",
      "data size =  4800\n",
      "data size =  4900\n",
      "data size =  5000\n",
      "data size =  5100\n",
      "data size =  5200\n",
      "data size =  5300\n",
      "data size =  5400\n",
      "data size =  5500\n",
      "data size =  5600\n",
      "data size =  5700\n",
      "data size =  5800\n",
      "data size =  5900\n",
      "data size =  6000\n",
      "data size =  6100\n",
      "data size =  6200\n",
      "data size =  6300\n",
      "data size =  6400\n",
      "data size =  6500\n",
      "data size =  6600\n",
      "data size =  6700\n",
      "data size =  6800\n",
      "data size =  6900\n",
      "data size =  7000\n",
      "data size =  7100\n",
      "data size =  7200\n",
      "data size =  7300\n",
      "data size =  7400\n",
      "data size =  7500\n",
      "data size =  7600\n",
      "data size =  7700\n",
      "data size =  7800\n",
      "data size =  7900\n",
      "data size =  8000\n",
      "data size =  8100\n",
      "data size =  8200\n",
      "data size =  8300\n",
      "data size =  8400\n",
      "data size =  8500\n",
      "data size =  8600\n",
      "data size =  8700\n",
      "data size =  8800\n",
      "data size =  8900\n",
      "data size =  9000\n",
      "data size =  9100\n",
      "data size =  9200\n",
      "data size =  9300\n",
      "data size =  9400\n",
      "data size =  9500\n",
      "data size =  9600\n",
      "data size =  9700\n",
      "data size =  9800\n",
      "data size =  9900\n",
      "data size =  10000\n",
      "data size =  10100\n",
      "data size =  10200\n",
      "data size =  10300\n",
      "data size =  10400\n",
      "data size =  10500\n",
      "data size =  10600\n",
      "data size =  10700\n",
      "data size =  10800\n",
      "data size =  10900\n",
      "data size =  11000\n",
      "data size =  11100\n",
      "data size =  11200\n",
      "data size =  11300\n",
      "data size =  11400\n",
      "data size =  11500\n",
      "data size =  11600\n",
      "data size =  11700\n",
      "data size =  11800\n",
      "data size =  11900\n",
      "data size =  12000\n",
      "data size =  12100\n",
      "data size =  12200\n",
      "data size =  12300\n",
      "data size =  12400\n",
      "data size =  12500\n",
      "data size =  12600\n",
      "data size =  12700\n",
      "data size =  12800\n",
      "data size =  12900\n",
      "data size =  13000\n",
      "data size =  13100\n",
      "data size =  13200\n",
      "data size =  13300\n",
      "data size =  13400\n",
      "data size =  13500\n",
      "data size =  13600\n",
      "data size =  13700\n",
      "data size =  13800\n",
      "data size =  13900\n",
      "data size =  14000\n",
      "data size =  14100\n",
      "data size =  14200\n",
      "data size =  14300\n",
      "data size =  14400\n",
      "data size =  14500\n",
      "data size =  14600\n",
      "data size =  14700\n",
      "data size =  14800\n",
      "data size =  14900\n",
      "data size =  15000\n",
      "data size =  15100\n",
      "data size =  15200\n",
      "data size =  15300\n",
      "data size =  15400\n",
      "data size =  15500\n",
      "data size =  15600\n",
      "data size =  15700\n",
      "data size =  15800\n",
      "data size =  15900\n",
      "data size =  16000\n",
      "data size =  16100\n",
      "data size =  16200\n",
      "data size =  16300\n",
      "data size =  16400\n",
      "data size =  16500\n",
      "data size =  16600\n",
      "data size =  16700\n",
      "data size =  16800\n",
      "data size =  16900\n",
      "data size =  17000\n",
      "data size =  17100\n",
      "data size =  17200\n",
      "data size =  17300\n",
      "data size =  17400\n",
      "data size =  17500\n",
      "data size =  17600\n",
      "data size =  17700\n",
      "data size =  17800\n",
      "data size =  17900\n",
      "data size =  18000\n",
      "data size =  18100\n",
      "data size =  18200\n",
      "data size =  18300\n",
      "data size =  18400\n",
      "data size =  18500\n",
      "data size =  18600\n",
      "data size =  18700\n",
      "data size =  18800\n",
      "data size =  18900\n",
      "data size =  19000\n",
      "data size =  19100\n",
      "data size =  19200\n",
      "data size =  19300\n",
      "data size =  19400\n",
      "data size =  19500\n",
      "data size =  19600\n",
      "data size =  19700\n",
      "data size =  19800\n",
      "data size =  19900\n",
      "data size =  20000\n",
      "data size =  20100\n",
      "data size =  20200\n",
      "data size =  20300\n",
      "data size =  20400\n",
      "data size =  20500\n",
      "data size =  20600\n",
      "data size =  20700\n",
      "data size =  20800\n",
      "data size =  20900\n",
      "data size =  21000\n",
      "data size =  21100\n",
      "data size =  21200\n",
      "data size =  21300\n",
      "data size =  21400\n",
      "data size =  21500\n",
      "data size =  21600\n",
      "data size =  21700\n",
      "data size =  21800\n",
      "data size =  21900\n",
      "data size =  22000\n",
      "data size =  22100\n",
      "data size =  22200\n",
      "data size =  22300\n",
      "data size =  22400\n",
      "data size =  22500\n",
      "data size =  22600\n",
      "data size =  22700\n",
      "data size =  22800\n",
      "data size =  22900\n",
      "data size =  23000\n",
      "data size =  23100\n",
      "data size =  23200\n",
      "data size =  23300\n",
      "data size =  23400\n",
      "data size =  23500\n",
      "data size =  23600\n",
      "data size =  23700\n",
      "data size =  23800\n",
      "data size =  23900\n",
      "data size =  24000\n",
      "data size =  24100\n",
      "data size =  24200\n",
      "data size =  24300\n",
      "data size =  24400\n",
      "data size =  24500\n",
      "data size =  24600\n",
      "data size =  24700\n",
      "data size =  24800\n",
      "data size =  24900\n",
      "data size =  25000\n",
      "data size =  25100\n",
      "data size =  25200\n",
      "data size =  25300\n",
      "data size =  25400\n",
      "data size =  25500\n",
      "data size =  25600\n",
      "data size =  25700\n",
      "data size =  25800\n",
      "data size =  25900\n",
      "data size =  26000\n",
      "data size =  26100\n",
      "data size =  26200\n",
      "data size =  26300\n",
      "data size =  26400\n",
      "data size =  26500\n",
      "data size =  26600\n",
      "data size =  26700\n",
      "data size =  26800\n",
      "data size =  26900\n",
      "data size =  27000\n",
      "data size =  27100\n",
      "data size =  27200\n",
      "data size =  27300\n",
      "data size =  27400\n",
      "data size =  27500\n",
      "data size =  27600\n",
      "data size =  27700\n",
      "data size =  27800\n",
      "data size =  27900\n",
      "data size =  28000\n",
      "data size =  28100\n",
      "data size =  28200\n",
      "data size =  28300\n",
      "data size =  28400\n",
      "data size =  28500\n",
      "data size =  28600\n",
      "data size =  28700\n",
      "data size =  28800\n",
      "data size =  28900\n",
      "data size =  29000\n",
      "data size =  29100\n",
      "data size =  29200\n",
      "data size =  29300\n",
      "data size =  29400\n",
      "data size =  29500\n",
      "data size =  29600\n",
      "data size =  29700\n",
      "data size =  29800\n",
      "data size =  29900\n",
      "data size =  30000\n",
      "data size =  30100\n",
      "data size =  30200\n",
      "data size =  30300\n",
      "data size =  30400\n",
      "data size =  30500\n",
      "data size =  30600\n",
      "data size =  30700\n",
      "data size =  30800\n",
      "data size =  30900\n"
     ]
    }
   ],
   "source": [
    "process.counter = 0\n",
    "process.file_index = 0\n",
    "thrds = []\n",
    "for i in range(0,(file_size//batch_size)+1):\n",
    "    #translate(str(i))\n",
    "    try:\n",
    "        print(\"processing file = \", str(i))\n",
    "        td =  threading.Thread(target=translate, args = (i,))\n",
    "        td.start()\n",
    "        thrds.append(td)\n",
    "       \n",
    "    except:\n",
    "        print(\"Error: unable to start thread\")\n",
    "\n",
    "for td in thrds:\n",
    "    td.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#12:20pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pprint(pretrained_data_processed[753118-21])\n",
    "cnt=0\n",
    "for dt in pretrained_data_processed:\n",
    "    cnt += (dt['status']=='delete')\n",
    "print(cnt)\n",
    "print(len(pretrained_data_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "BASE_code=\"/media/amllab-2070/WIN_10/research_stuff/codes/\"\n",
    "\n",
    "def get_source_target(file1, file2, max_target_len = 5, max_input_len=20):\n",
    "    difftype ='alldiff'\n",
    "    source_target = subprocess.check_output(['java', '-jar', 'AllDIff.jar',\\\n",
    "                                     difftype,file1, file2, str(max_target_len), \\\n",
    "                                     str(max_input_len)]).decode('utf-8')\n",
    "    return source_target \n",
    "#source_target = get_source_target(BASE_code+'acumos_11/2.java',BASE_code+'acumos_11/3.java')\n",
    "source_target = get_source_target('prev_code.txt','changed_code.txt').replace(\"alldiff\", \"\")\n",
    "#print(lst)\n",
    "for i in source_target.split('<|datasep|>'):\n",
    "    print(i)\n",
    "    #lst = i.split('\\n')\n",
    "    #print(lst)\n",
    "    #print('###############')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_processed[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Thread Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_processed = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    process(unique_data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Change Trigger Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def read(path):\n",
    "    return open(path, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "\n",
    "BASE=\"C:/research_stuff/codes\"\n",
    "unique_data = getJsonData('E:/APR/DATA/unique_data_processed_with_Date_multithread_idx_400_feb_03.json')\n",
    "print(unique_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getJsonData('unique_data_processed_with_Date_multithread.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(data[0]['source'])\n",
    "print(\"===================\")\n",
    "print(data[0]['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_file_name = unique_data[i]['file_name']\n",
    "base_patch_number = unique_data[i]['base_patch_number']\n",
    "changed_patch_number = unique_data[i]['changed_patch_number']\n",
    "main_code_path = BASE+code_file_name+'/'+str(base_patch_number)+'.java'\n",
    "changed_code_path = BASE+code_file_name+'/'+str(changed_patch_number)+'.java'\n",
    "line_no = unique_data[i]['line_number']\n",
    "main_code = read(main_code_path)\n",
    "changed_code = read(changed_code_path)\n",
    "sample = main_code\n",
    "message = unique_data[i]['message']\n",
    "comment_id = unique_data[i]['comment_id']\n",
    "source = \"\"\n",
    "target=''\n",
    "source, target = get_source_target(main_code_path, changed_code_path,line_no-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(message)\n",
    "print(\"######################\")\n",
    "print(target)\n",
    "print(\"######################\")\n",
    "print(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(unique_data_processed[i]['message'])\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[i]['target'])\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[i]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code = unique_data_processed[30]\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main_code_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_processed[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_processed[0]['target'][:-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "i = random.randint(0,len(unique_data))\n",
    "\n",
    "unique_data_processed = []\n",
    "process(unique_data[i])\n",
    "\n",
    "print(\"Data Number: \", i)\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[0]['status'])\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[0]['message'])\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[0]['target'])\n",
    "print(\"##################################\")\n",
    "print(unique_data_processed[0]['code_snippet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_data_processed[0]['func_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data_processed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data2 = unique_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for i in range(len(unique_data2)):\n",
    "    unique_data2[i]['global_index'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_data2[100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unique_data_with_date_and_index.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(unique_data2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Data, Removing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tufano_test_50 = [13257, 10443, 21152, 8344, 6123, 6122, 21086, 9006, 14137, 2523, 20560, 19915, 11901, 21693, 11107, 2862, 17879, 12346, 14420, 3304, 20658, 13924, 21678, 1945, 19047, 4809, 16525, 21206, 14443, 21481, 5892, 13829, 13432, 5099, 4386, 11997, 19914, 12053, 2593, 10330, 15305, 11388, 3721, 20969, 5081, 1698, 10571, 18025, 15679, 17847, 11928, 11603, 4889, 2275, 2486, 19685, 5751, 8624, 7797, 17666, 7386, 3637, 5322, 15897, 19141, 18152, 9304, 10875, 11265, 10798, 4037, 20348, 1523, 11467, 8681, 3425, 9065, 3218, 10253, 13353, 2653, 14120, 16091, 2254, 7186, 6130, 8591, 9508, 5079, 2852, 13805, 11996, 20144, 13898, 8802, 14704, 18353, 5013, 10893, 21737, 2146, 14135, 8558, 12660, 11119, 14087, 3666, 10010, 21986, 11362, 13777, 14591, 9078, 16886, 11084, 8497, 21973, 3012, 6120, 10343, 11429, 12975, 16540, 6691, 15168, 17916, 12147, 4926, 11437, 13825, 19745, 21110, 2824, 11167, 3949, 10816, 10579, 15102, 13452, 9525, 3361, 5752, 11120, 8512, 17598, 13134, 3389, 17098, 12098, 20887, 12875, 11401, 10811, 19452, 11042, 20528, 18278, 8801, 7395, 7634, 13026, 14750, 12598, 2134, 15328, 19022, 17539, 18903, 20678, 17284, 2698, 19401, 8613, 21682, 13847, 15378, 8760, 19787, 12348, 6266, 5039, 9425, 12811, 16796, 12824, 4066, 8307, 2391, 4808, 17597, 20049, 14315, 13013, 15698, 2851, 2778, 17500, 13803, 6042, 20000, 3333, 18905, 2113, 17724, 8840, 20665, 21543, 10030, 14929, 10877, 14961, 3141, 17846, 13344, 10153, 5940, 12649, 14890, 14759, 21421, 10709, 3897, 3007, 20834, 21107, 7277, 11720, 11016, 3132, 10841, 17746, 20477, 4076, 16242, 21175, 10139, 16452, 10316, 15003, 9490, 20271, 20888, 7093, 19349, 9007, 6101, 11181, 14823, 12253, 1996, 18145, 14751, 9972, 9596, 12088, 19747, 11083, 9567, 12603, 11750, 5546, 4447, 4172, 10231, 1650, 2514, 6627, 18772, 16202, 5078, 13562, 19487, 14082, 11099, 20145, 8017, 2859, 20371, 5300, 4534, 10822, 10089, 7806, 17574, 17865, 7658, 20993, 12764, 18782, 19060, 7917, 17068, 19092, 16548, 14880, 19254, 2046, 5105, 20178, 6343, 12439, 18995, 21093, 7637, 18155, 11978, 17798, 12613, 20820, 21919, 8003, 21297, 6125, 14429, 20179, 15169, 6131, 12874, 18760, 1402, 5474, 20450, 2408, 10968, 4168, 13885, 11620, 10380, 1518, 14043, 2093, 12839, 17671, 1764, 4088, 18904, 2433, 17884, 9189, 8978, 8953, 18781, 1467, 14927, 13354, 16361, 11101, 10873, 18126, 15104, 16655, 9019, 20718, 12065, 14791, 20213, 15955, 17754, 18763, 21618, 21132, 7185, 5684, 4455, 14855, 5415, 7214, 14058, 4021, 18969, 4820, 15936, 8065, 3724, 11624, 16604, 20508, 7767, 15567, 14923, 16698, 6243, 9487, 12865, 2864, 11176, 13093, 2860, 5379, 2879, 12283, 19458, 17392, 7718, 4585, 21436, 17247, 5705, 4735, 2386]\n",
    "tufano_test_100 = [5465, 15764, 18264, 11017, 17135, 7848, 21592, 15676, 4869, 20105, 15796, 12010, 6956, 13149, 3673, 5991, 21519, 13763, 7374, 16092, 5043, 12551, 14357, 17860, 17187, 13954, 7030, 6202, 3467, 13696, 10996, 10205, 16827, 17144, 7037, 13859, 8467, 14869, 10480, 4893, 8712, 19648, 11063, 10037, 10857, 3957, 14268, 11575, 8316, 3794, 10844, 20940, 11465, 15763, 13030, 6917, 12043, 21798, 17757, 18194, 19258, 4191, 16231, 18081, 9406, 18148, 18118, 15061, 2886, 21260, 11596, 15238, 6756, 12052, 18150, 4086, 7969, 3802, 15128, 4891, 6204, 6094, 21280, 13208, 3800, 12828, 15638, 6190, 5044, 7581, 11108, 7463, 21699, 12292, 12012, 20156, 21525, 16585, 13428, 11331, 11973, 18106, 21605, 4316, 4193, 9350, 8132, 16085, 4081, 17251, 10370, 5124, 12350, 16364, 9090, 8274, 12494, 5494, 4687, 19477, 8572, 16612, 9294, 8711, 4196, 18593, 2124, 7180, 1831, 4438, 19840, 18159, 15787, 16490, 10023, 20247, 18416, 5083, 4697, 18808, 14070, 13494, 8973, 10344, 13591, 19593, 19282, 21673, 12521, 3485, 15253, 5872, 19947, 3465, 10281, 1813, 7368, 13593, 8174, 7208, 6921, 15096, 21011, 9628, 13234, 14074, 14183, 15081, 18103, 8879, 10803, 15959, 13000, 8700, 19198, 5565, 7218, 16129, 11793, 20778, 21560, 5336, 7042, 11242, 18499, 12548, 13612, 5020, 10331, 7885, 2544, 8972, 14452, 3015, 3114, 1748, 9349, 14412, 2565, 4170, 3026, 20135, 21397, 6314, 17076, 20741, 17688, 6694, 16411, 17143, 20033, 10314, 16232, 20161, 3335, 2277, 9411, 2123, 16420, 18312, 4154, 20672, 5019, 8628, 17760, 12749, 20394, 19806, 3058, 12474, 18299, 1820, 15313, 4827, 3370, 15144, 18121, 8573, 4085, 11187, 19948, 17911, 2590, 14870, 11705, 6870, 11654, 16197, 10479, 14051, 12321, 1552, 7239, 2104, 18160, 14986, 10442, 16412, 9450, 10686, 9053, 4821, 6395, 1545, 5694, 8936, 12944, 3977, 7842, 1757, 2644, 6869, 6765, 14451, 9992, 17012, 18331, 6779, 13637, 12173, 10576, 14858, 10516, 7426, 5115, 15713, 2681, 6075, 15603, 21981, 7259, 8503, 7850, 5965, 15727, 8719, 10587, 3630, 20031, 13826, 18817, 9471, 11749, 5048, 11974, 3985, 6758, 9347, 3881, 13521, 5181, 20011, 13934, 13809, 15623, 6211, 17495, 6778, 6191, 15502, 16858, 11014, 7260, 14354, 11856, 2586, 19851, 5356, 20594, 16856, 13276, 7308, 16449, 13305, 15417, 12493, 2888, 9054, 4437, 4892, 19975, 12880, 3911, 14444, 9263, 12051, 7571, 11824, 12135, 21252, 16170, 21654, 15948, 13267, 5495, 6926, 3412, 19478, 5164, 12659, 16047, 2106, 9728, 21580, 12217, 19857, 7608, 15222, 15249, 7613, 19476, 11062, 12684, 2887, 3431, 13733, 12947, 4023, 6955, 11226, 11259, 5726, 11879, 16857, 8814, 16203, 17026, 15139, 12464, 13476, 1926, 14057, 14316, 12627, 4080, 5126, 10382, 1873, 12296, 20347]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import traceback\n",
    "\n",
    "def merge_code_comment(x):\n",
    "    message = x['message']\n",
    "    source = ' '.join(x['tokenized_code_snippet'])\n",
    "    target = ' '.join(x['target'])\n",
    "    merged = message + '<>' + source + '<>' + target\n",
    "    return merged\n",
    "\n",
    "existing_ids = set()\n",
    "duplicate_ids = set()\n",
    "unique_dat = []\n",
    "i = 0\n",
    "target_size = 400\n",
    "error_count = 0\n",
    "error_list = []\n",
    "\n",
    "for x in unique_data:\n",
    "    #print(dt['idx'])\n",
    "    #x = unique_data[dt['idx']]\n",
    "    i+= 1\n",
    "    try:\n",
    "        focuslen = x['tokenized_code_snippet'].index('<|endfocus|>') - x['tokenized_code_snippet'].index('<|startfocus|>')\n",
    "        key = merge_code_comment(x)\n",
    "        if key not in existing_ids and len(x['tokenized_target']) <= target_size and focuslen <= target_size:\n",
    "            existing_ids.add(key)\n",
    "            unique_dat.append(i-1)\n",
    "        elif key not in duplicate_ids:\n",
    "            duplicate_ids.add(key)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        x['error'] = e\n",
    "        error_list.append(x)\n",
    "        error_count += 1\n",
    "        #print(x)\n",
    "        #break\n",
    "print(len(unique_dat))\n",
    "\n",
    "\n",
    "global2loval = {}\n",
    "local2global = {}\n",
    "for i, d in enumerate(unique_data):\n",
    "    global2loval[d['global_index']] = i\n",
    "    local2global[i] = d['global_index']\n",
    "\n",
    "\n",
    "training_data = []\n",
    "\n",
    "for i in unique_dat: #1:75\n",
    "    dt = unique_data[i]\n",
    "    #pprint(dt)\n",
    "    if dt['global_index'] not in tufano_test_50 and dt['global_index'] not in tufano_test_100:\n",
    "        '''data = {}\n",
    "        data['global_index'] = dt['global_index']\n",
    "        data['code_snippet'] =dt['code_snippet'] \n",
    "        data['tokenized_code_snippet']=dt['tokenized_code_snippet']\n",
    "        data['message']=dt['message']\n",
    "        data['tokenized_comment']=dt['tokenized_comment']\n",
    "        data['target']=dt['target']\n",
    "        data['tokenized_target']=dt['tokenized_target']'''\n",
    "        training_data.append(dt)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_data.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(training_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = getJsonData('unique_data_processed_with_multithread_v2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "changed_training_data = []\n",
    "statuses = ['none', 'unchanged']\n",
    "for x in data:\n",
    "    if x['status'] in statuses:\n",
    "        continue\n",
    "    changed_training_data.append(x)\n",
    "print(len(changed_training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changed_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_project_names = []\n",
    "for x in unique_data:\n",
    "    if x['project_name'] not in unique_project_names:\n",
    "        unique_project_names.append(x['project_name'])\n",
    "print(unique_project_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "# Date wise calculation\n",
    "oct19 = datetime.strptime('2018-01-01 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def to_integer(str_date_time):\n",
    "    dt_time = datetime.strptime(str_date_time, '%Y-%m-%d %H:%M:%S')\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n",
    "\n",
    "#    date_string = x['written_on']\n",
    "#    date = datetime.strptime(date_string, '%Y-%m-%d %H:%M:%S')\n",
    "#    if date > oct19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(changed_training_data)):\n",
    "    str_date = changed_training_data[i]['written_on']\n",
    "    changed_training_data[i]['int_date'] = to_integer(str_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "changed_training_data[0]['int_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_wise_test = {}\n",
    "for x in unique_project_names:\n",
    "    project_wise_test[x] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in changed_training_data:\n",
    "    project = x['project_name']\n",
    "    project_wise_test[project].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('all_data_with_date_integer.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(changed_training_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split 5% as testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_by_project = []\n",
    "test_by_project = []\n",
    "for x in project_wise_test:\n",
    "    print(x, len(project_wise_test[x]))\n",
    "    #shuffle(project_wise_test[x])\n",
    "    project_wise_test[x] = sorted(project_wise_test[x], key=lambda k: k['int_date'])\n",
    "    num_test = len(project_wise_test[x])//20\n",
    "    #for p in project_wise_test[x][:100]:\n",
    "    #    print(p['int_date'])\n",
    "    train_by_project = train_by_project + project_wise_test[x][:len(project_wise_test[x])-num_test]\n",
    "    test_by_project = test_by_project + project_wise_test[x][-num_test:]\n",
    "    '''\n",
    "    for i in range(len(project_wise_test[x])):\n",
    "        if i<num_test:\n",
    "            test_by_project.append(project_wise_test[x][i])\n",
    "        else:\n",
    "            train_by_project.append(project_wise_test[x][i])\n",
    "    '''\n",
    "print(len(train_by_project), len(test_by_project))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('train_by_project.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(train_by_project, f)\n",
    "\n",
    "import json\n",
    "with open('test_by_project.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(test_by_project, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_project[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of total data: 85594\n",
    "Number of changed data: 68535\n",
    "none: 652\n",
    "unchanged: 16407\n",
    "insert: 5262\n",
    "delete: 11828\n",
    "update: 51445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2, 3]\n",
    "b = [5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_wise_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "def read(path):\n",
    "    return open(path, 'r', encoding = 'utf-8').read()\n",
    "\n",
    "\n",
    "BASE=\"C:/research_stuff/codes/\"\n",
    "unique = getJsonData('E:/APR/DATA/unique_data_with_date_and_index.json')\n",
    "unique_data = getJsonData('unique_data_processed_with_Date_multithread_idx_400_feb_06_v2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx , dt in enumerate(unique_data):\n",
    "    if(len(dt['tokenized_code_snippet']))>400:\n",
    "        #print(unique_data['global_index'])\n",
    "        #print(\"index = \", unique_data[idx]['global_index'])\n",
    "        #print(len(process(unique[unique_data[idx]['global_index']])['tokenized_code_snippet']))\n",
    "        if len(process(unique[unique_data[idx]['global_index']])['tokenized_code_snippet']) >400:\n",
    "            print(\"index = \", unique_data[idx]['global_index'], len(process(unique[unique_data[idx]['global_index']])['tokenized_code_snippet']))\n",
    "#process(unique_data[545])['code_snippet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(unique_data))\n",
    "ct=0\n",
    "for data in unique_data:\n",
    "    if data['code_snippet'][:14] == \"<|startfocus|>\" or data['code_snippet'][-13:] == \"\\n<|endfocus|>\":\n",
    "        ct+=1\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_code_comment(x):\n",
    "    message = x['message']\n",
    "    source = ' '.join(x['tokenized_code_snippet'])\n",
    "    target = ' '.join(x['target'])\n",
    "    merged = message + '<>' + source + '<>' + target\n",
    "    return merged\n",
    "\n",
    "existing_ids = set()\n",
    "duplicate_ids = set()\n",
    "unique_dat = []\n",
    "i = 0\n",
    "target_size = 400\n",
    "error_count = 0\n",
    "error_list = []\n",
    "\n",
    "for x in unique_data:\n",
    "    #print(dt['idx'])\n",
    "    #x = unique_data[dt['idx']]\n",
    "    i+= 1\n",
    "    try:\n",
    "        focuslen = x['tokenized_code_snippet'].index('<|endfocus|>') - x['tokenized_code_snippet'].index('<|startfocus|>')\n",
    "        key = merge_code_comment(x)\n",
    "        if key not in existing_ids and len(x['tokenized_target']) <= target_size and focuslen <= target_size:\n",
    "            existing_ids.add(key)\n",
    "            unique_dat.append(x)\n",
    "        elif key not in duplicate_ids:\n",
    "            duplicate_ids.add(key)\n",
    "    except Exception as e:\n",
    "        #print(e)\n",
    "        #x['error'] = e\n",
    "        error_list.append(x)\n",
    "        error_count += 1\n",
    "        #print(x)\n",
    "        #break\n",
    "print(len(unique_dat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(len(unique_dat))\n",
    "ct=0\n",
    "for data in unique_dat:\n",
    "    if data['code_snippet'][:14] == \"<|startfocus|>\" or data['code_snippet'][-13:] == \"\\n<|endfocus|>\":\n",
    "        ct+=1\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "train = getJsonData('train_by_project.json')\n",
    "test = getJsonData('test_by_project.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "print(train[0].keys()==test[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = train+test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_data_for_sequencer.json', 'w', encoding=\"utf8\") as f:  # writing JSON object\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getJsonData(JsonFile):\n",
    "    with open(JsonFile, encoding=\"utf8\") as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "all_data = getJsonData('all_data_for_sequencer.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for dt in all_data:\n",
    "    if 'del' in dt['target']:\n",
    "        cnt+=1\n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
